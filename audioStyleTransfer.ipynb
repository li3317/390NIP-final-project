{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbuhPxEyvt3z"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "# from scipy.optimize import fmin_l_bfgs_b   # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html\n",
        "from tensorflow.keras.applications import vgg19\n",
        "import warnings\n",
        "\n",
        "random.seed(1618)\n",
        "np.random.seed(1618)\n",
        "# tf.set_random_seed(1618)   # Uncomment for TF1.\n",
        "tf.random.set_seed(1618)\n",
        "\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)   # Uncomment for TF1.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "PROJECT_PATH = \"drive/My Drive/NIP Final Project Audio/\"\n",
        "CONTENT_FILE_NAME = \"AliceInWonderLandShort.wav\"             #Reference: https://www.youtube.com/watch?v=uUcJSTpMavA\n",
        "CONTENT_AUD_PATH  = PROJECT_PATH+CONTENT_FILE_NAME           #DONE: Add Content Path. \n",
        "STYLE_FILE_NAME   = \"TheLittlePrinceShort.wav\"               #Reference: https://www.youtube.com/watch?v=yWQo_AAHDUA\n",
        "STYLE_AUD_PATH    = PROJECT_PATH+STYLE_FILE_NAME             #DONE: Add Style Path. \n",
        "\n",
        "CONTENT_CSV_EXIST = False\n",
        "STYLE_CSV_EXIST = False\n",
        "\n",
        "NNFT = 512        #Default Value Currently\n",
        "WIN_LENGTH = NNFT #Default Value Currently\n",
        "HOP_LENGTH = WIN_LENGTH // 4\n",
        "N_FILTERS = 4096\n",
        "\n",
        "CONTENT_WEIGHT = 1e-6    # Alpha weight.\n",
        "STYLE_WEIGHT = 3.5e-5      # Beta weight.\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#=============================<Helper Fuctions>=================================\n",
        "\n",
        "def gramMatrix(x):\n",
        "    # features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = np.matmul(x.T, x) / N_SAMPLES\n",
        "    return gram\n",
        "\n",
        "def displayAudioSpectrum(frequencyMagnitude, samplingRate, fileName):\n",
        "    fig, ax = plt.subplots()\n",
        "    librosa.display.specshow(librosa.power_to_db(frequencyMagnitude, ref=np.max), sr=samplingRate, hop_length = HOP_LENGTH, y_axis='mel', x_axis='time', cmap = cm.jet)\n",
        "    ax.set(title = \"Content: \"+fileName)\n",
        "    fig.savefig(\"drive/My Drive/NIP Final Project Audio/\"+fileName+\".jpg\")\n",
        "    print(\"Spectrum of \"+fileName+\" saved\")\n",
        "\n",
        "#========================<Loss Function Builder Functions>======================\n",
        "\n",
        "def styleLoss(style_gram, g_gram, N_CHANNELS, N_SAMPLES):\n",
        "    return K.sum(K.square(tf.nn.l2_loss(g_gram - style_gram)) / (4. * (N_FILTERS ** 2) * (N_CHANNELS * N_SAMPLES) ** 2))   #DONE: implement styleLoss, change numFilter to correct variable.\n",
        "    # Reference: Slide 8\n",
        "\n",
        "def contentLoss(content, gen):\n",
        "    return K.sum(K.square(tf.nn.l2_loss(gen - content)))\n",
        "\n",
        "\n",
        "# def totalLoss(x): # designed to keep the generated image locally coherent. Reference: https://keras.io/examples/generative/neural_style_transfer/\n",
        "#     a = K.square(x[:, : CONTENT_IMG_H - 1, : CONTENT_IMG_W - 1, :] - x[:, 1 : , : CONTENT_IMG_W - 1, :])\n",
        "#     b = K.square(x[:, : CONTENT_IMG_H - 1, : CONTENT_IMG_W - 1, :] - x[:, : CONTENT_IMG_W - 1, 1 : , :])\n",
        "#     return K.sum(K.pow(a+b,1.25))   #DONE: implement total varient loss.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=========================<Pipeline Functions>==================================\n",
        "\n",
        "\n",
        "\n",
        "def getRawData():\n",
        "    global CONTENT_CSV_EXIST\n",
        "    global STYLE_CSV_EXIST\n",
        "    print(\"   Loading Audios.\")\n",
        "    print(\"      Content audio URL:  \\\"%s\\\".\" % CONTENT_AUD_PATH)\n",
        "    print(\"      Style audio URL:    \\\"%s\\\".\" % STYLE_AUD_PATH)\n",
        "    # librosa libray loads amplitude and sampling rate (HZ) of the audio file (amplitude vs time)\n",
        "    try:\n",
        "        cFrequencyMagnitude = np.load(CONTENT_AUD_PATH+'FrequencyMagnitude.npy')\n",
        "        cSrFile = open(CONTENT_AUD_PATH+'SamplingRate.txt', 'r')\n",
        "        cSamplingRate = int(cSrFile.readline())\n",
        "        cSrFile.close()\n",
        "        CONTENT_CSV_EXIST = True\n",
        "        print(\"Content Raw Data Exist\")\n",
        "    except IOError:\n",
        "        cAmplitude, cSamplingRate = librosa.load(CONTENT_AUD_PATH) \n",
        "\n",
        "    try:\n",
        "        sFrequencyMagnitude = np.load(STYLE_AUD_PATH+'FrequencyMagnitude.npy')\n",
        "        sSrFile = open(STYLE_AUD_PATH+'SamplingRate.txt', 'r')\n",
        "        sSamplingRate = int(sSrFile.readline())\n",
        "        sSrFile.close()\n",
        "        STYLE_CSV_EXIST = True\n",
        "        print(\"Style Raw Data Exist\")\n",
        "    except IOError:\n",
        "        sAmplitude, sSamplingRate = librosa.load(STYLE_AUD_PATH)\n",
        "        print(\"      Audios have been loaded.\")\n",
        "\n",
        "    \"\"\" Reference: http://man.hubwiz.com/docset/LibROSA.docset/Contents/Resources/Documents/generated/librosa.core.stft.html\n",
        "    frequency conversion from (amplitude vs time) to (power vs frequency)\n",
        "\n",
        "    function stft returns a complex-valued matrix D such that:\n",
        "        np.abs(D[f, t]) is the magnitude of frequency bin f at frame t\n",
        "        np.angle(D[f, t]) is the phase of frequency bin f at frame t\n",
        "\n",
        "    Parameters: \n",
        "        n_fft:\n",
        "            recommended value is 512, \n",
        "            replaces all the period larger than win_length to zero paddings to improve frequency resolution by TTF \n",
        "        hop_length:\n",
        "            defaultly equal to win_length // 4,\n",
        "            actual length without overlaping in the window\n",
        "        win_length:\n",
        "            defaultly equal to n_fft,\n",
        "            the length of window each ttf will be done\n",
        "    \"\"\"\n",
        "    if not (CONTENT_CSV_EXIST):\n",
        "        cSTFT = librosa.stft(cAmplitude, NNFT, HOP_LENGTH, WIN_LENGTH)\n",
        "        print(\"      STFT Conversion for Cotent Completed.\")\n",
        "        cFrequencyMagnitude = np.abs(cSTFT)\n",
        "        np.save(CONTENT_AUD_PATH+'FrequencyMagnitude.npy', cFrequencyMagnitude)\n",
        "        cSrFile = open(CONTENT_AUD_PATH+'SamplingRate.txt','w')  # w : writing mode  /  r : reading mode  /  a  :  appending mode \n",
        "        cSrFile.write('{}'.format(cSamplingRate))   \n",
        "        cSrFile.close() #Reference: https://stackoverflow.com/questions/36900443/how-to-format-the-file-output-python-3\n",
        "        print(\"Raw Data files for Cotent saved\")\n",
        "    if not (STYLE_CSV_EXIST): \n",
        "        sSTFT = librosa.stft(sAmplitude, NNFT, HOP_LENGTH, WIN_LENGTH)\n",
        "        print(\"      STFT Conversion for Style Completed.\")\n",
        "        sFrequencyMagnitude = np.abs(sSTFT)\n",
        "        np.save(STYLE_AUD_PATH+'FrequencyMagnitude.npy',sFrequencyMagnitude)\n",
        "        sSrFile = open(STYLE_AUD_PATH+'SamplingRate.txt','w')  # w : writing mode  /  r : reading mode  /  a  :  appending mode\n",
        "        sSrFile.write('{}'.format(sSamplingRate))\n",
        "        sSrFile.close()\n",
        "        print(\"Raw Data files for Style saved\")\n",
        "\n",
        "    \"\"\" reference: https://librosa.org/doc/main/auto_examples/plot_display.html\n",
        "        Display audio spectrum\n",
        "    \"\"\"\n",
        "    if not (CONTENT_CSV_EXIST):\n",
        "        displayAudioSpectrum(cFrequencyMagnitude, cSamplingRate, CONTENT_FILE_NAME)\n",
        "    if not (STYLE_CSV_EXIST): \n",
        "        displayAudioSpectrum(sFrequencyMagnitude, sSamplingRate, STYLE_FILE_NAME)\n",
        "\n",
        "    \"\"\"\n",
        "        check if data is loaded correctly\n",
        "    \"\"\"\n",
        "    # print(\"Content Sampling Rate: \" + str(cSamplingRate))\n",
        "    # print(\"Content Frequncy Magnitude: \")\n",
        "    # print(cFrequencyMagnitude)\n",
        "    # print(\"Style Sampling Rate: \" + str(sSamplingRate))\n",
        "    # print(\"Style Frequncy Magnitude: \")\n",
        "    # print(sFrequencyMagnitude)\n",
        "\n",
        "    return ((cFrequencyMagnitude, cSamplingRate), (sFrequencyMagnitude, sSamplingRate))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocessData(raw):\n",
        "    frequencyMagnitude, samplingRate = raw\n",
        "    return np.log1p(frequencyMagnitude) # use log recieve numerical stability\n",
        "    \n",
        "\n",
        "\n",
        "def styleTransfer(cData, sData, cSamplingRate, sSamplingRate):\n",
        "    cTensor = K.variable(cData)\n",
        "    sTensor = K.variable(sData)\n",
        "    gTensor = K.placeholder(shape=cTensor.shape)\n",
        "    N_CHANNELS = cTensor.shape[0]\n",
        "    N_SAMPLES  = cTensor.shape[1]\n",
        "    sTensor = sTensor[:N_CHANNELS, :N_SAMPLES] # cut the size of style to fit the size of content\n",
        "    print(\"Shape of Content Tensor: %s.\" % str(cTensor.shape))\n",
        "    print(\"Shape of Style Tensor: %s.\" % str(sTensor.shape))\n",
        "    print(\"Shape of Generating Tensor: %s.\" % str(gTensor.shape))\n",
        "    print(\"N_CHANNELS: \"+str(N_CHANNELS))\n",
        "    print(\"N_SAMPLES: \"+str(N_SAMPLES))\n",
        "    \n",
        "    \"\"\" \n",
        "    Reference: https://www.tensorflow.org/guide/effective_tf2\n",
        "    tf.nn network creation:\n",
        "        all previous tf.nn layer can be inserted into next layer as input\n",
        "\n",
        "    Reference: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
        "    tf.nn.conv2d layer:\n",
        "        Parameters: \n",
        "            input: A Tensor of type (half, bfloat16, float32, float64)\n",
        "            filters: A Tensor of same type as input, shape (filter_height, filter_width, in_channels, out_channels)\n",
        "            strides: list of ints, the stride of the sliding window for each dimension of input\n",
        "            padding: \"SAME\" or \"VALID\"\n",
        "        Example:\n",
        "            x_in = np.array([[\n",
        "                [[2], [1], [2], [0], [1]],\n",
        "                [[1], [3], [2], [2], [3]],\n",
        "                [[1], [1], [3], [3], [0]],\n",
        "                [[2], [2], [0], [1], [1]],\n",
        "                [[0], [0], [3], [1], [2]], ]])\n",
        "            kernel_in = np.array([\n",
        "                [ [[2, 0.1]], [[3, 0.2]] ],\n",
        "                [ [[0, 0.3]],[[1, 0.4]] ], ])\n",
        "            x = tf.constant(x_in, dtype=tf.float32)\n",
        "            kernel = tf.constant(kernel_in, dtype=tf.float32)\n",
        "            tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='VALID')\n",
        "    \"\"\"\n",
        "\n",
        "    kernel_in = np.random.randn(1, 11, N_CHANNELS, N_FILTERS)\n",
        "    kernel = tf.constant(kernel_in, dtype='float32')\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "      cNet = tf.nn.conv2d(cData.T[None,None,:,:].astype(np.float32), kernel, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
        "      cNet = tf.nn.relu(cNet)\n",
        "      cEval = cNet.eval()\n",
        "\n",
        "      sNet = tf.nn.conv2d(sData.T[None,None,:,:].astype(np.float32), kernel, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
        "      sNet = tf.nn.relu(sNet)\n",
        "      sEval = sNet.eval()\n",
        "\n",
        "      gNet = tf.nn.conv2d(np.random.randn(1,1,N_SAMPLES,N_CHANNELS).astype(np.float32), kernel, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
        "      gNet = tf.nn.relu(gNet)\n",
        "\n",
        "      style_features = np.reshape(sEval, (-1, N_FILTERS))\n",
        "      g_features = np.reshape(gNet.eval(), (-1, N_FILTERS))\n",
        "      \n",
        "      opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "      x = tf.Variable(np.zeros((1,1, N_SAMPLES, N_CHANNELS)))\n",
        "\n",
        "      print(\"gram calculation complete\")\n",
        "      content_loss = CONTENT_WEIGHT * contentLoss(cEval,gNet.eval())\n",
        "      print(\"content loss calculation complete\")\n",
        "      style_loss = STYLE_WEIGHT * styleLoss(gramMatrix(style_features),gramMatrix(g_features),N_CHANNELS,N_SAMPLES)\n",
        "      print(\"style loss calculation complete\")\n",
        "      total_loss =  tf.cast(style_loss,content_loss.dtype) + content_loss # Reference: https://stackoverflow.com/questions/35725513/tensorflow-cast-a-float64-tensor-to-float32\n",
        "      print(\"loss calculation complete\")\n",
        "      print(total_loss.eval())\n",
        "           \n",
        "\n",
        "\n",
        "\n",
        "      opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "    \n",
        "      sess.run(tf.compat.v1.initialize_all_variables())\n",
        "      print('Started optimization.')\n",
        "      opt.minimize(sess, x)\n",
        "      print('Final loss:')\n",
        "      print(total_loss.eval())\n",
        "      result = x.eval()\n",
        "\n",
        "\n",
        "#=========================<Main>================================================\n",
        "\n",
        "def main():\n",
        "    print(\"Starting style transfer program.\")\n",
        "    raw = getRawData()                                  #TODO: Get raw Data of two audio\n",
        "    cData = preprocessData(raw[0])   # Content image.   #TODO: Preprocess Data of Content Audio\n",
        "    sData = preprocessData(raw[1])   # Style image.     #TODO: Preprocess Data of Style Audio \n",
        "    styleTransfer(cData, sData, raw[0][1], raw[1][1])\n",
        "    print(\"Done. Goodbye.\")\n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting style transfer program.\n",
            "   Loading Audios.\n",
            "      Content audio URL:  \"drive/My Drive/NIP Final Project Audio/AliceInWonderLandShort.wav\".\n",
            "      Style audio URL:    \"drive/My Drive/NIP Final Project Audio/TheLittlePrinceShort.wav\".\n",
            "Content Raw Data Exist\n",
            "Style Raw Data Exist\n",
            "Shape of Content Tensor: (257, 103360).\n",
            "Shape of Style Tensor: (257, 103360).\n",
            "Shape of Generating Tensor: (257, 103360).\n",
            "N_CHANNELS: 257\n",
            "N_SAMPLES: 103360\n",
            "gram calculation complete\n",
            "content loss calculation complete\n",
            "style loss calculation complete\n",
            "loss calculation complete\n",
            "566880100.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Started optimization.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'Session' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d494919476da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-d494919476da>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mcData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Content image.   #TODO: Preprocess Data of Content Audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0msData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Style image.     #TODO: Preprocess Data of Style Audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mstyleTransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done. Goodbye.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d494919476da>\u001b[0m in \u001b[0;36mstyleTransfer\u001b[0;34m(cData, sData, cSamplingRate, sSamplingRate)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Started optimization.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final loss:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m    374\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 375\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Session' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import vgg19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 6s 0us/step\n"
          ]
        }
      ],
      "source": [
        "    model = vgg19.VGG19(include_top=False, weights=\"imagenet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg19\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         [(None, None, None, 3)]   0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n_________________________________________________________________\nblock3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n_________________________________________________________________\nblock4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n_________________________________________________________________\nblock5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n=================================================================\nTotal params: 20,024,384\nTrainable params: 20,024,384\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}