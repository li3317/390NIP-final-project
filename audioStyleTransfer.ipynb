{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbuhPxEyvt3z"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "# from scipy.optimize import fmin_l_bfgs_b   # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html\n",
        "# from tensorflow.keras.applications import vgg19\n",
        "import warnings\n",
        "\n",
        "random.seed(1618)\n",
        "np.random.seed(1618)\n",
        "# tf.set_random_seed(1618)   # Uncomment for TF1.\n",
        "tf.random.set_seed(1618)\n",
        "\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)   # Uncomment for TF1.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "PROJECT_PATH = \"drive/My Drive/NIP Final Project Audio/\"\n",
        "CONTENT_FILE_NAME = \"AliceInWonderLandShort.wav\"             #Reference: https://www.youtube.com/watch?v=uUcJSTpMavA\n",
        "CONTENT_AUD_PATH  = PROJECT_PATH+CONTENT_FILE_NAME           #DONE: Add Content Path. \n",
        "STYLE_FILE_NAME   = \"TheLittlePrinceShort.wav\"               #Reference: https://www.youtube.com/watch?v=yWQo_AAHDUA\n",
        "STYLE_AUD_PATH    = PROJECT_PATH+STYLE_FILE_NAME             #DONE: Add Style Path. \n",
        "\n",
        "CONTENT_CSV_EXIST = False\n",
        "STYLE_CSV_EXIST = False\n",
        "\n",
        "NNFT = 512        #Default Value Currently\n",
        "WIN_LENGTH = NNFT #Default Value Currently\n",
        "HOP_LENGTH = WIN_LENGTH // 4\n",
        "\n",
        "#=============================<Helper Fuctions>=================================\n",
        "\n",
        "def gramMatrix(x):\n",
        "    # features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features, K.transpose(features))\n",
        "    return gram\n",
        "\n",
        "def displayAudioSpectrum(frequencyMagnitude, samplingRate, fileName):\n",
        "    fig, ax = plt.subplots()\n",
        "    librosa.display.specshow(librosa.power_to_db(frequencyMagnitude, ref=np.max), sr=samplingRate, hop_length = HOP_LENGTH, y_axis='mel', x_axis='time', cmap = cm.jet)\n",
        "    ax.set(title = \"Content: \"+fileName)\n",
        "    fig.savefig(\"drive/My Drive/NIP Final Project Audio/\"+fileName+\".jpg\")\n",
        "    print(\"Spectrum of \"+fileName+\" saved\")\n",
        "\n",
        "#========================<Loss Function Builder Functions>======================\n",
        "\n",
        "def styleLoss(style, gen):\n",
        "    return K.sum(K.square(gramMatrix(style) - gramMatrix(gen)) / (4. * (numFilter ** 2) * (STYLE_IMG_H * STYLE_IMG_W) ** 2))   #DONE: implement styleLoss, change numFilter to correct variable.\n",
        "    # Reference: Slide 8\n",
        "\n",
        "def contentLoss(content, gen):\n",
        "    return K.sum(K.square(gen - content))\n",
        "\n",
        "\n",
        "def totalLoss(x): # designed to keep the generated image locally coherent. Reference: https://keras.io/examples/generative/neural_style_transfer/\n",
        "    a = K.square(x[:, : CONTENT_IMG_H - 1, : CONTENT_IMG_W - 1, :] - x[:, 1 : , : CONTENT_IMG_W - 1, :])\n",
        "    b = K.square(x[:, : CONTENT_IMG_H - 1, : CONTENT_IMG_W - 1, :] - x[:, : CONTENT_IMG_W - 1, 1 : , :])\n",
        "    return K.sum(K.pow(a+b,1.25))   #DONE: implement total varient loss.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=========================<Pipeline Functions>==================================\n",
        "\n",
        "\n",
        "\n",
        "def getRawData():\n",
        "    global CONTENT_CSV_EXIST\n",
        "    global STYLE_CSV_EXIST\n",
        "    print(\"   Loading Audios.\")\n",
        "    print(\"      Content audio URL:  \\\"%s\\\".\" % CONTENT_AUD_PATH)\n",
        "    print(\"      Style audio URL:    \\\"%s\\\".\" % STYLE_AUD_PATH)\n",
        "    # librosa libray loads amplitude and sampling rate (HZ) of the audio file (amplitude vs time)\n",
        "    try:\n",
        "        cFrequencyMagnitude = np.load(CONTENT_AUD_PATH+'FrequencyMagnitude.npy')\n",
        "        cSrFile = open(CONTENT_AUD_PATH+'SamplingRate.txt', 'r')\n",
        "        cSamplingRate = int(cSrFile.readline())\n",
        "        cSrFile.close()\n",
        "        CONTENT_CSV_EXIST = True\n",
        "        print(\"Content Raw Data Exist\")\n",
        "    except IOError:\n",
        "        cAmplitude, cSamplingRate = librosa.load(CONTENT_AUD_PATH) \n",
        "\n",
        "    try:\n",
        "        sFrequencyMagnitude = np.load(STYLE_AUD_PATH+'FrequencyMagnitude.npy')\n",
        "        sSrFile = open(STYLE_AUD_PATH+'SamplingRate.txt', 'r')\n",
        "        sSamplingRate = int(sSrFile.readline())\n",
        "        sSrFile.close()\n",
        "        STYLE_CSV_EXIST = True\n",
        "        print(\"Style Raw Data Exist\")\n",
        "    except IOError:\n",
        "        sAmplitude, sSamplingRate = librosa.load(STYLE_AUD_PATH)\n",
        "        print(\"      Audios have been loaded.\")\n",
        "\n",
        "    \"\"\" Reference: http://man.hubwiz.com/docset/LibROSA.docset/Contents/Resources/Documents/generated/librosa.core.stft.html\n",
        "    frequency conversion from (amplitude vs time) to (power vs frequency)\n",
        "\n",
        "    function stft returns a complex-valued matrix D such that:\n",
        "        np.abs(D[f, t]) is the magnitude of frequency bin f at frame t\n",
        "        np.angle(D[f, t]) is the phase of frequency bin f at frame t\n",
        "\n",
        "    Parameters: \n",
        "        n_fft:\n",
        "            recommended value is 512, \n",
        "            replaces all the period larger than win_length to zero paddings to improve frequency resolution by TTF \n",
        "        hop_length:\n",
        "            defaultly equal to win_length // 4,\n",
        "            actual length without overlaping in the window\n",
        "        win_length:\n",
        "            defaultly equal to n_fft,\n",
        "            the length of window each ttf will be done\n",
        "    \"\"\"\n",
        "    if not (CONTENT_CSV_EXIST):\n",
        "        cSTFT = librosa.stft(cAmplitude, NNFT, HOP_LENGTH, WIN_LENGTH)\n",
        "        print(\"      STFT Conversion for Cotent Completed.\")\n",
        "        cFrequencyMagnitude = np.abs(cSTFT)\n",
        "        np.save(CONTENT_AUD_PATH+'FrequencyMagnitude.npy', cFrequencyMagnitude)\n",
        "        cSrFile = open(CONTENT_AUD_PATH+'SamplingRate.txt','w')  # w : writing mode  /  r : reading mode  /  a  :  appending mode \n",
        "        cSrFile.write('{}'.format(cSamplingRate))   \n",
        "        cSrFile.close() #Reference: https://stackoverflow.com/questions/36900443/how-to-format-the-file-output-python-3\n",
        "        print(\"Raw Data files for Cotent saved\")\n",
        "    if not (STYLE_CSV_EXIST): \n",
        "        sSTFT = librosa.stft(sAmplitude, NNFT, HOP_LENGTH, WIN_LENGTH)\n",
        "        print(\"      STFT Conversion for Style Completed.\")\n",
        "        sFrequencyMagnitude = np.abs(sSTFT)\n",
        "        np.save(STYLE_AUD_PATH+'FrequencyMagnitude.npy',sFrequencyMagnitude)\n",
        "        sSrFile = open(STYLE_AUD_PATH+'SamplingRate.txt','w')  # w : writing mode  /  r : reading mode  /  a  :  appending mode\n",
        "        sSrFile.write('{}'.format(sSamplingRate))\n",
        "        sSrFile.close()\n",
        "        print(\"Raw Data files for Style saved\")\n",
        "\n",
        "    \"\"\" reference: https://librosa.org/doc/main/auto_examples/plot_display.html\n",
        "        Display audio spectrum\n",
        "    \"\"\"\n",
        "    if not (CONTENT_CSV_EXIST):\n",
        "        displayAudioSpectrum(cFrequencyMagnitude, cSamplingRate, CONTENT_FILE_NAME)\n",
        "    if not (STYLE_CSV_EXIST): \n",
        "        displayAudioSpectrum(sFrequencyMagnitude, sSamplingRate, STYLE_FILE_NAME)\n",
        "\n",
        "    \"\"\"\n",
        "        check if data is loaded correctly\n",
        "    \"\"\"\n",
        "    # print(\"Content Sampling Rate: \" + str(cSamplingRate))\n",
        "    # print(\"Content Frequncy Magnitude: \")\n",
        "    # print(cFrequencyMagnitude)\n",
        "    # print(\"Style Sampling Rate: \" + str(sSamplingRate))\n",
        "    # print(\"Style Frequncy Magnitude: \")\n",
        "    # print(sFrequencyMagnitude)\n",
        "\n",
        "    return ((cFrequencyMagnitude, cSamplingRate), (sFrequencyMagnitude, sSamplingRate))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocessData(raw):\n",
        "    frequencyMagnitude, samplingRate = raw\n",
        "    return np.log1p(frequencyMagnitude) # use log recieve numerical stability\n",
        "    \n",
        "\n",
        "\n",
        "def styleTransfer(cData, sData, cSamplingRate, sSamplingRate):\n",
        "    cTensor = K.variable(cData)\n",
        "    sTensor = K.variable(sData)\n",
        "    gTensor = K.placeholder(shape=cTensor.shape)\n",
        "    cTensor = tf.expand_dims(cTensor, axis=0)\n",
        "    cTensor = tf.expand_dims(cTensor, axis=0)\n",
        "    print(\"Shape of Content Tensor: %s.\" % str(cTensor.shape))\n",
        "    print(\"Shape of Style Tensor: %s.\" % str(sTensor.shape))\n",
        "    print(\"Shape of Generating Tensor: %s.\" % str(gTensor.shape))\n",
        "    print(\"Content Loss: \")\n",
        "    print(contentLoss(cTensor,gTensor))\n",
        "    print(styleLoss(sTensor,gTensor))\n",
        "#=========================<Main>================================================\n",
        "\n",
        "def main():\n",
        "    print(\"Starting style transfer program.\")\n",
        "    raw = getRawData()                                  #TODO: Get raw Data of two audio\n",
        "    cData = preprocessData(raw[0])   # Content image.   #TODO: Preprocess Data of Content Audio\n",
        "    sData = preprocessData(raw[1])   # Style image.     #TODO: Preprocess Data of Style Audio \n",
        "    styleTransfer(cData, sData, raw[0][1], raw[1][1])\n",
        "    print(\"Done. Goodbye.\")\n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting style transfer program.\n",
            "   Loading Audios.\n",
            "      Content audio URL:  \"drive/My Drive/NIP Final Project Audio/AliceInWonderLandShort.wav\".\n",
            "      Style audio URL:    \"drive/My Drive/NIP Final Project Audio/TheLittlePrinceShort.wav\".\n",
            "Content Raw Data Exist\n",
            "Style Raw Data Exist\n",
            "Shape of Content Tensor: (1, 1, 257, 103360).\n",
            "Shape of Style Tensor: (257, 103360).\n",
            "Shape of Generating Tensor: (257, 103360).\n",
            "Content Loss: \n",
            "Tensor(\"Sum_4:0\", shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-033a70c42970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-033a70c42970>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mcData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Content image.   #TODO: Preprocess Data of Content Audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0msData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Style image.     #TODO: Preprocess Data of Style Audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mstyleTransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done. Goodbye.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-033a70c42970>\u001b[0m in \u001b[0;36mstyleTransfer\u001b[0;34m(cData, sData, cSamplingRate, sSamplingRate)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content Loss: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontentLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyleLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;31m#=========================<Main>================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-033a70c42970>\u001b[0m in \u001b[0;36mstyleLoss\u001b[0;34m(style, gen)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstyleLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgramMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgramMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumFilter\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSTYLE_IMG_H\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mSTYLE_IMG_W\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#DONE: implement styleLoss, change numFilter to correct variable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Reference: Slide 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-033a70c42970>\u001b[0m in \u001b[0;36mgramMatrix\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgramMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mpermute_dimensions\u001b[0;34m(x, pattern)\u001b[0m\n\u001b[1;32m   2941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2942\u001b[0m   \"\"\"\n\u001b[0;32m-> 2943\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name, conjugate)\u001b[0m\n\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2188\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m  11528\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11529\u001b[0m       return transpose_eager_fallback(\n\u001b[0;32m> 11530\u001b[0;31m           x, perm, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m  11531\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11532\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose_eager_fallback\u001b[0;34m(x, perm, name, ctx)\u001b[0m\n\u001b[1;32m  11553\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tperm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tperm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11554\u001b[0m   _result = _execute.execute(b\"Transpose\", 1, inputs=_inputs_flat,\n\u001b[0;32m> 11555\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m  11556\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11557\u001b[0m     _execute.record_gradient(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    }
  ]
}